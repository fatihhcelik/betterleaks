#!/usr/bin/env python3
"""
Generate a slim English word list for the Betterleaks word filter.

Uses the wordfreq library (frequency-based data from Open Subtitles, Wikipedia, etc.)
instead of the full NLTK corpus, so we keep commonly used words and drop exotic/rare
ones (e.g. "aal", "aalii", long "z" words). Outputs Go code for words/words.go.

Run from repo root:
  uv run python words/slim_words.py              # overwrites words/words.go
  uv run python words/slim_words.py -o out.go    # write to out.go

Tuning:
  --min-zipf 2.5   (default) ~1 per 300k words; use 3.0 or 3.5 for a stricter list.
  --max-len 14     cap length to avoid very long rare words.
"""

import argparse
import sys
from pathlib import Path

# Defaults tuned for secret-detection word filter: common words only, avoid very long rare words.
MIN_LEN = 3
MAX_LEN = 14  # e.g. skip "10 character z words" and other long exotic words
MIN_ZIPF = 2.5  # ~1 per 300k words; keeps "password", "github", drops "aalii", "aam", etc.
WORDLIST = "large"  # "large" = 10^-8 and higher; "small" = 10^-6 and higher
LANG = "en"


def main() -> None:
    parser = argparse.ArgumentParser(description="Slim word list for Betterleaks (frequency-based).")
    parser.add_argument(
        "--output", "-o",
        type=Path,
        default=None,
        help="Output Go file path (default: words/words.go next to this script)",
    )
    parser.add_argument(
        "--min-len",
        type=int,
        default=MIN_LEN,
        help=f"Minimum word length (default: {MIN_LEN})",
    )
    parser.add_argument(
        "--max-len",
        type=int,
        default=MAX_LEN,
        help=f"Maximum word length (default: {MAX_LEN})",
    )
    parser.add_argument(
        "--min-zipf",
        type=float,
        default=MIN_ZIPF,
        help=f"Minimum Zipf frequency (default: {MIN_ZIPF}); higher = rarer words excluded",
    )
    parser.add_argument(
        "--wordlist",
        choices=("small", "large", "best"),
        default=WORDLIST,
        help="wordfreq wordlist (default: large)",
    )
    args = parser.parse_args()
    if args.output is None:
        args.output = Path(__file__).resolve().parent / "words.go"

    try:
        from wordfreq import iter_wordlist, zipf_frequency
    except ImportError:
        print("Run: uv sync  (or pip install wordfreq)", file=sys.stderr)
        sys.exit(1)

    words: list[str] = []
    for w in iter_wordlist(LANG, args.wordlist):
        if len(w) < args.min_len or len(w) > args.max_len:
            continue
        if not w.isascii() or not w.isalpha():
            continue  # letters only; skip "0,0", "0.0", etc.
        if zipf_frequency(w, LANG, args.wordlist) < args.min_zipf:
            continue
        words.append(w)

    words.sort()
    write_go_map(args.output, words)
    print(f"Wrote {len(words)} words to {args.output}", file=sys.stderr)


def write_go_map(path: Path, words: list[str]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w") as f:
        f.write("package words\n\n")
        f.write("// nltkWords contains common English words (>= 3 chars) from wordfreq.\n")
        f.write("// This file is auto-generated by words/slim_words.py (frequency-based).\n")
        f.write("var nltkWords = map[string]struct{}{\n")
        for w in words:
            f.write(f'\t"{w}": {{}},\n')
        f.write("}\n")


if __name__ == "__main__":
    main()
